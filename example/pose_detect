from __future__ import division
import argparse, time, logging, os, math, tqdm, cv2

import numpy as np
import mxnet as mx
from mxnet import gluon, nd, image
from mxnet.gluon.data.vision import transforms

import matplotlib.pyplot as plt

import gluoncv as gcv
from gluoncv import data
from gluoncv.data import mscoco
from gluoncv.model_zoo import get_model
from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord
from gluoncv.utils.viz import cv_plot_image, cv_plot_keypoints
# This article will demonstrate how to estimate people's pose from your webcam video stream.
import math


def my_cv_plot_keypoints(img, coords, confidence, class_ids, bboxes, scores,
                      box_thresh=0.5, keypoint_thresh=0.2, scale=1.0, **kwargs):
    """Visualize keypoints with OpenCV.

    Parameters
    ----------
    img : numpy.ndarray or mxnet.nd.NDArray
        Image with shape `H, W, 3`.
    coords : numpy.ndarray or mxnet.nd.NDArray
        Array with shape `Batch, N_Joints, 2`.
    confidence : numpy.ndarray or mxnet.nd.NDArray
        Array with shape `Batch, N_Joints, 1`.
    class_ids : numpy.ndarray or mxnet.nd.NDArray
        Class IDs.
    bboxes : numpy.ndarray or mxnet.nd.NDArray
        Bounding boxes with shape `N, 4`. Where `N` is the number of boxes.
    scores : numpy.ndarray or mxnet.nd.NDArray, optional
        Confidence scores of the provided `bboxes` with shape `N`.
    box_thresh : float, optional, default 0.5
        Display threshold if `scores` is provided. Scores with less than `box_thresh`
        will be ignored in display.
    keypoint_thresh : float, optional, default 0.2
        Keypoints with confidence less than `keypoint_thresh` will be ignored in display.
    scale : float
        The scale of output image, which may affect the positions of boxes

    Returns
    -------
    numpy.ndarray
        The image with estimated pose.

    """
    import matplotlib.pyplot as plt

    import cv2

    if isinstance(img, mx.nd.NDArray):
        img = img.asnumpy()
    if isinstance(coords, mx.nd.NDArray):
        coords = coords.asnumpy()
    if isinstance(class_ids, mx.nd.NDArray):
        class_ids = class_ids.asnumpy()
    if isinstance(bboxes, mx.nd.NDArray):
        bboxes = bboxes.asnumpy()
    if isinstance(scores, mx.nd.NDArray):
        scores = scores.asnumpy()
    if isinstance(confidence, mx.nd.NDArray):
        confidence = confidence.asnumpy()

    joint_visible = confidence[:, :, 0] > keypoint_thresh
    joint_pairs = [[0, 1], [1, 3], [0, 2], [2, 4],
                   [5, 6], [5, 7], [7, 9], [6, 8], [8, 10],
                   [5, 11], [6, 12], [11, 12],
                   [11, 13], [12, 14], [13, 15], [14, 16]]

    person_ind = class_ids[0] == 0
    # img = cv_plot_bbox(img, bboxes[0][person_ind[:, 0]], scores[0][person_ind[:, 0]],
    #                    thresh=box_thresh, class_names='person', scale=scale, **kwargs)

    colormap_index = np.linspace(0, 1, len(joint_pairs))
    coords *= scale
    for i in range(coords.shape[0]):
        pts = coords[i]

        left_eye = (int(pts[1][0]),int(pts[1][1]))
        right_eye = (int(pts[2][0]),int(pts[2][1]))
        eye_angle = cmp_angle(left_eye, right_eye)


        #求角度
        # angle_eye = np.tan((pts[1][0],pts[1][1]),(pts[2][0],pts[2][1]))


        left_shouder = (int(pts[5][0]), int(pts[5][1]))
        right_shouder = (int(pts[6][0]), int(pts[6][1]))
        shouder_angle = cmp_angle(left_shouder,right_shouder)
        if eye_angle  and shouder_angle:
            print("zuozizhengque",eye_angle,shouder_angle)
            cv2.putText(img,' position Right',(50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (125, 255, 255), 2)
        else:
            print("ni zuo wai le ")
            cv2.putText(img, 'Wrong', (50, 50),cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 124, 255), 2)
        cv2.line(img, left_shouder, right_shouder, (112,0,0), 3)


    return img


def cmp_angle(left_eye, right_eye):
    """
    计算两点之间坐标
    :param left_eye:
    :type left_eye:
    :param right_eye:
    :type right_eye:
    :return:
    :rtype:
    """
    if left_eye[1] == right_eye[1]:
        # print("正确", 0)
        return True
    elif (right_eye[0] == left_eye[0]):
        # print("x", right_eye, left_eye)
        return True
    else:
        k = (right_eye[1] - left_eye[1]) / (right_eye[0] - left_eye[0])
        angle_eye = np.arctan(k) * 57.29577
        print("angle ", angle_eye)
        if int(angle_eye) > 1:
            # print("不正")
            return False
        else:
            return True


def fitler_boxes(bounding_boxs,scores, class_IDs):
    default_person = 1
    new_bounding_boxs =[]
    new_boxxes = []
    new_classId = []
    new_scores = []
    bboxes = bounding_boxs[0].asnumpy()
    # 阈值
    thresh = 0.9
    scores = scores[0].asnumpy()
    class_IDs = class_IDs[0].asnumpy()
    id = []
    sc = []
    for i, bbox in enumerate(bboxes):
        if scores is not None and scores.flat[i] < thresh:
            continue
        id.append(class_IDs[i])
        sc.append(scores[i])
        new_boxxes.append(bbox.tolist())


    new_scores.append(sc)
    new_classId.append(id)
    new_bounding_boxs.append(new_boxxes)


    return nd.array(np.asarray(new_classId)),nd.array(np.asarray(new_scores)),nd.array(np.asarray(new_bounding_boxs))

ctx = mx.cpu()
detector_name = "ssd_512_mobilenet1.0_coco"
detector = get_model(detector_name, pretrained=True, ctx=ctx,root= './pose_model/')




detector.reset_class(classes=['person'], reuse_weights={'person':'person'})
detector.hybridize()

estimator = get_model('simple_pose_resnet18_v1b', pretrained='ccd24037', ctx=ctx,root='./pose_model')
estimator.hybridize()


cap = cv2.VideoCapture(0)
time.sleep(1)  ### letting the camera autofocus





axes = None
num_frames = 100

for i in range(num_frames):
    ret, frame = cap.read()
    frame = mx.nd.array(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).astype('uint8')

    x, frame = gcv.data.transforms.presets.ssd.transform_test(frame, short=512, max_size=350)
    x = x.as_in_context(ctx)
    class_IDs, scores, bounding_boxs = detector(x)

    #对boxes 过滤
    class_IDs,scores,bounding_boxs = fitler_boxes(bounding_boxs,scores,class_IDs)



    pose_input, upscale_bbox = detector_to_simple_pose(frame, class_IDs, scores, bounding_boxs,
                                                           output_shape=(128, 96), ctx=ctx)
    # print("pose input ",pose_input)
    img = None
    if len(upscale_bbox) > 0:
        predicted_heatmap = estimator(pose_input)
        pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox)

        img = my_cv_plot_keypoints(frame, pred_coords, confidence, class_IDs, bounding_boxs, scores,
                                    box_thresh=0.7, keypoint_thresh=0.2)
    if img is None:
        continue
    cv_plot_image(img)
    cv2.waitKey(1)



cap.release()



